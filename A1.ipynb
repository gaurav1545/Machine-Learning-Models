{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlimitedDataWorks:\n",
    "\n",
    "    def __init__(self, deg):\n",
    "        self.exp = []\n",
    "        for i in range(deg+1):\n",
    "            for j in range(deg+1):\n",
    "                if i+j <= deg:\n",
    "                    self.exp.append((i, j))\n",
    "\n",
    "    def train_test_split(self, dataframe):\n",
    "        normalize = lambda x: ((x - x.min()) / (x.max() - x.min()))\n",
    "        self.data = pd.DataFrame([])\n",
    "        self.count = -1\n",
    "        for (a, b) in self.exp:\n",
    "            self.count += 1\n",
    "            res = ((dataframe[\"lat\"] ** a) * (dataframe[\"lon\"] ** b))\n",
    "            self.data.insert(self.count, \"col\" + str(a) + str(b), res, True)\n",
    "\n",
    "        self.count += 1 \n",
    "        dataframe = normalize(dataframe)\n",
    "        self.data = normalize(self.data)\n",
    "        self.data[\"col00\"] = [1.0]*len(self.data)\n",
    "        \n",
    "        # generate a 70-20-10 split on the data:\n",
    "        X = self.data[:304113]\n",
    "        Y = dataframe[\"alt\"][:304113]\n",
    "        xval = self.data[304113:391088]\n",
    "        yval = dataframe[\"alt\"][304113:391088]\n",
    "        x = self.data[391088:]\n",
    "        y = dataframe[\"alt\"][391088:]   \n",
    "        return (X, Y, xval, yval, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    def __init__(self, N, X, Y, x, y, xval, yval):\n",
    "        \"\"\"\n",
    "        X :: training data                  (304113 x 3)\n",
    "        x :: testing data                   (43786 x 3)\n",
    "        Y :: training target values         (304113 x 1)\n",
    "        y :: testing target values          (43786 x 1)\n",
    "        xval :: validation data             (86975 x 3)\n",
    "        yval :: validation training data    (86975 X 1)\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.X = np.array(X)\n",
    "        self.Y = np.array(Y)\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        self.xval = np.array(xval)\n",
    "        self.yval = np.array(yval)\n",
    "\n",
    "    def score(self, weights):\n",
    "        \"\"\"\n",
    "        the following method helps us find the\n",
    "        R2 (R-squared) error of a given training data\n",
    "        wrt the generated weights\n",
    "        \"\"\"\n",
    "        ss_tot = sum(np.square(np.mean(self.y) - self.y))\n",
    "        ss_res = sum(np.square((self.x @ weights) - self.y))\n",
    "        rmse = sqrt(ss_res/len(self.x))\n",
    "        r2 = (1-(ss_res / ss_tot))\n",
    "        return [r2*100, rmse]\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        \"\"\"\n",
    "        train till error is almost constant\n",
    "        \"\"\"\n",
    "        lr = 8e-7\n",
    "        prev_err, count = 1e10, 0\n",
    "        W = np.random.randn(self.N)\n",
    "        while True:\n",
    "            diff = ((self.X @ W) - self.Y)\n",
    "            err = 0.5 * (diff @ diff)\n",
    "            grad = (self.X.T @ diff)\n",
    "            if count % 500 == 0:\n",
    "                    print(\"epoch =\", count, \"| err_diff =\", prev_err-err)\n",
    "                    print(\"error = \", err, \"||\", W)\n",
    "                    print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "            W -= lr * grad\n",
    "            if abs(prev_err-err) <= 1e-4:\n",
    "                break\n",
    "            prev_err = err\n",
    "            count += 1\n",
    "        print(count, err)\n",
    "        print(W, self.score(W), end=\"\\n\\n\")\n",
    "\n",
    "    def stocastic_gradient_descent(self, epochs):\n",
    "        \"\"\"\n",
    "        train till error is almost constant\n",
    "        \"\"\"\n",
    "        lr = 0.05\n",
    "        W = np.random.randn(self.N)\n",
    "        for count in range(epochs):\n",
    "            diff = ((self.X @ W) - self.Y)\n",
    "            err = 0.5 * (diff @ diff)\n",
    "            W -= lr * (((self.X[count] @ W) - self.Y[count]) * self.X[count])\n",
    "            if count % 500 == 0:\n",
    "                print(\"epoch =\", count)\n",
    "                print(\"error =\", err, \"||\", W)\n",
    "                print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "\n",
    "    def gradient_descent_L1_reg(self):\n",
    "        \"\"\"\n",
    "        attempts a L1 regularization on the data\n",
    "        considering 10% of training data as validation data\n",
    "        \"\"\"\n",
    "        W_fin = np.array([])\n",
    "        lr, l1_fin = 5e-7, 0\n",
    "        MVLE = 1e10\n",
    "        L1_vals = [0.0, 0.05, 0.15, 0.25, 0.35, 0.45,\n",
    "                   0.55, 0.65, 0.75, 0.85, 0.95, 1.0]\n",
    "        sgn = lambda x: (x / abs(x))\n",
    "        for l1 in L1_vals:\n",
    "            prev_err, count = 1e10, 0\n",
    "            W = np.random.randn(self.N)\n",
    "            while True:\n",
    "                diff = ((self.X @ W) - self.Y)\n",
    "                err = 0.5 * ((diff @ diff) + l1*sum([abs(w) for w in W]))\n",
    "                if count % 500 == 0:\n",
    "                    print(\"L1 hyperparamter =\", l1, end=\", \")\n",
    "                    print(\"epoch =\", count, \"| err_diff =\", prev_err-err)\n",
    "                    print(\"error = \", err, \"||\", W)\n",
    "                    print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "                sgn_w = np.array([sgn(w) for w in W])\n",
    "                W -= lr * ((self.X.T @ diff) + 0.5*l1*sgn_w)\n",
    "                if abs(prev_err-err) <= 0.005:\n",
    "                    break\n",
    "                prev_err = err\n",
    "                count += 1\n",
    "            VLD = ((self.xval @ W) - self.yval)\n",
    "            VLE = 0.5 * ((VLD.T @ VLD) + l1*sum([abs(w) for w in W]))\n",
    "            if VLE < MVLE:\n",
    "                W_fin = W\n",
    "                l1_fin = l1\n",
    "                MVLE = VLE\n",
    "        print(MVLE, l1_fin, W_fin)\n",
    "\n",
    "    def gradient_descent_L2_reg(self):\n",
    "        \"\"\"\n",
    "        attempts a L2 regularization on the data\n",
    "        considering 10% of training data as validation data\n",
    "        \"\"\"\n",
    "        W_fin = np.array([])\n",
    "        lr, l2_fin = 5e-7, 0\n",
    "        MVLE = 1e10\n",
    "        L2_vals = [0.0, 0.05, 0.15, 0.25, 0.35, 0.45,\n",
    "                   0.55, 0.65, 0.75, 0.85, 0.95, 1.0]\n",
    "        for l2 in L2_vals:\n",
    "            prev_err, count = 1e10, 0\n",
    "            W = np.random.randn(self.N)\n",
    "            while True:\n",
    "                diff = ((self.X @ W) - self.Y)\n",
    "                err = 0.5 * ((diff @ diff) + l2*sum([w*w for w in W]))\n",
    "                if count % 500 == 0:\n",
    "                    print(\"L2 hyperparamter =\", l2, end=\", \")\n",
    "                    print(\"epoch =\", count, \"| err_diff =\", prev_err-err)\n",
    "                    print(\"error = \", err, \"||\", W)\n",
    "                    print(\"score =\", self.score(W), end=\"\\n\\n\")\n",
    "                W -= lr * ((self.X.T @ diff) + l2*W)\n",
    "                if abs(prev_err-err) <= 0.005:\n",
    "                    break\n",
    "                prev_err = err\n",
    "                count += 1\n",
    "            VLD = ((self.xval @ W) - self.yval)\n",
    "            VLE = 0.5 * ((VLD.T @ VLD) + l2 * (W.T @ W))\n",
    "            if VLE < MVLE:\n",
    "                W_fin = W\n",
    "                l2_fin = l2\n",
    "                MVLE = VLE\n",
    "        print(MVLE, l2_fin, W_fin)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        solves for optimal weights using system of\n",
    "        N linear equations; AW = B, hence, W = inv(A)*B\n",
    "        \"\"\"\n",
    "        B = self.X.T @ self.Y\n",
    "        A = self.X.T @ self.X\n",
    "        W = (np.linalg.inv(A)) @ B\n",
    "        print(W, self.score(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"junk\", \"lat\", \"lon\", \"alt\"]\n",
    "raw_df = pd.read_csv(\"3D_spatial_network.txt\", sep=',', header=None,\n",
    "                     names=columns).drop(\"junk\", 1).sample(frac=1)\n",
    "\n",
    "pre_processor = UnlimitedDataWorks(deg=1)\n",
    "X_train, Y_train, x_val, y_val, x_test, y_test = pre_processor.train_test_split(raw_df)\n",
    "\n",
    "model = RegressionModel(N=pre_processor.count,\n",
    "                        X=X_train,\n",
    "                        Y=Y_train,\n",
    "                        x=x_test,\n",
    "                        y=y_test,\n",
    "                        xval=x_val,\n",
    "                        yval=y_val)\n",
    "\n",
    "model.fit()\n",
    "# model.gradient_descent()\n",
    "model.stocastic_gradient_descent(50250)\n",
    "# model.gradient_descent_L1_reg()\n",
    "# model.gradient_descent_L2_reg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
